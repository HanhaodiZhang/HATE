{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled34.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxBLF2P4UfCG58lEkQpiVd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanhaodiZhang/HATE/blob/main/%E8%BD%B4%E6%89%BF%E6%A3%80%E6%B5%8BCNN%E4%BB%A3%E7%A0%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYsIdbNhSgOp"
      },
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA \n",
        "from pandas.core.frame import DataFrame\n",
        "import torch\n",
        "import torch.utils.data.dataset as Dataset\n",
        "import torch.utils.data.dataloader as DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "path = r'data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtKkClIGSjX5"
      },
      "source": [
        "#读数据名\n",
        "filenames = os.listdir(r'data')\n",
        "files = {}\n",
        "#读取每个数据\n",
        "for i in filenames:\n",
        "  #路径\n",
        "  file_path = os.path.join(r'data', i)\n",
        "  #加载\n",
        "  file = loadmat(file_path)\n",
        "  #一个文件中不同类型数据\n",
        "  file_keys = file.keys()\n",
        "  for key in file_keys:\n",
        "    #选择；有DE的数据\n",
        "    if 'DE' in key:\n",
        "      #整理成一行一维数据\n",
        "      files[i] = file[key].ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DNhklReTGmQ"
      },
      "source": [
        "#一个数据1.400\n",
        "length=400\n",
        "#数据取值步长400，就是第一个1-400，第二个400-800\n",
        "step=400\n",
        "#标签从0开始\n",
        "label=0\n",
        "#建立数据集空白的\n",
        "#训练集\n",
        "df_train = pd.DataFrame(columns=('data', 'label','name'))\n",
        "#验证\n",
        "df_val = pd.DataFrame(columns=('data', 'label','name'))\n",
        "#测试\n",
        "df_test = pd.DataFrame(columns=('data', 'label','name'))\n",
        "keys = files.keys()\n",
        "#keys是不同标签的分类\n",
        "for i in keys:\n",
        "  #全部\n",
        "  slice_data = files[i]\n",
        "  #长度\n",
        "  all_lenght = len(slice_data)\n",
        "  #训练结束节点\n",
        "  train_end_index = int(all_lenght*0.6)\n",
        "  #验证结束节点\n",
        "  val_end_index = int(all_lenght*0.8)\n",
        "  start_index=0\n",
        "  #训练结束节点之前切数据成400\n",
        "  while start_index+length <= train_end_index:\n",
        "      #切\n",
        "      sample = slice_data[start_index:start_index + length]\n",
        "      #加400\n",
        "      start_index+=step\n",
        "      #加到数据集上\n",
        "      df_train=df_train.append([{'data':sample,'label':label,'name':i}], ignore_index=True)\n",
        "  \n",
        "  #注释同上\n",
        "  start_index=train_end_index    \n",
        "  while start_index+length <= val_end_index:\n",
        "      sample = slice_data[start_index:start_index + length]\n",
        "      start_index+=step\n",
        "      \n",
        "      df_val=df_val.append([{'data':sample,'label':label,'name':i}], ignore_index=True)\n",
        "      \n",
        "  start_index=val_end_index    \n",
        "  while start_index+length <= all_lenght:\n",
        "      sample = slice_data[start_index:start_index + length]\n",
        "      start_index+=step      \n",
        "      df_test=df_test.append([{'data':sample,'label':label,'name':i}], ignore_index=True)\n",
        "  \n",
        "  label+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxSv1RnUYeO7"
      },
      "source": [
        "打乱数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZwtoOu_YWPb"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "df_train = shuffle(df_train)\n",
        "df_test = shuffle(df_test)\n",
        "df_val = shuffle(df_val)\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "df_val.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8i_-T4GZ1gB"
      },
      "source": [
        "数据可视化（你不需要注释吧）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu9MLh7ZZ0j_"
      },
      "source": [
        "noramlarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "train_nomal=df_train[df_train['label']==1]\n",
        "for arry in train_nomal['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  noramlarry=np.append(noramlarry, arry, axis=0)\n",
        "\n",
        "gundongarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "train_gundong=df_train[df_train['label']==2]\n",
        "for arry in train_gundong['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  gundongarry=np.append(gundongarry, arry, axis=0)\n",
        "\n",
        "outarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "train_out=df_train[df_train['label']==3]\n",
        "for arry in train_out['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  outarry=np.append(outarry, arry, axis=0)\n",
        "\n",
        "inarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "train_in=df_train[df_train['label']==0]\n",
        "for arry in train_in['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  inarry=np.append(inarry, arry, axis=0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiYPEu9RZSzb"
      },
      "source": [
        "allarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "for arry in df_train['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  allarry=np.append(allarry, arry, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0qYnAyJaBi5"
      },
      "source": [
        "sklearn.decomposition.PCA\n",
        "pca = PCA(n_components=3)\n",
        "pca=pca.fit(allarry)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzjDpBsnaEKX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(1, figsize=(8, 8))\n",
        "plt.clf()\n",
        "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
        "plt.cla()\n",
        "\n",
        "pca1=pca.fit(noramlarry)\n",
        "X_1 = pca1.transform(noramlarry)\n",
        "pca2=pca.fit(gundongarry)\n",
        "X_2 = pca2.transform(gundongarry)\n",
        "\n",
        "pca3=pca.fit(inarry)\n",
        "X_3 = pca3.transform(inarry)\n",
        "pca4=pca.fit(outarry)\n",
        "X_4 = pca4.transform(outarry)\n",
        "x1=ax.scatter(X_1[:, 0], X_1[:, 1], X_1[:, 2],c='#1f77b4',s=10, marker=\"o\", edgecolor='face')\n",
        "x2=ax.scatter(X_2[:, 0], X_2[:, 1], X_2[:, 2],c='#2ca02c',s=10, marker=\"x\", edgecolor='face')\n",
        "x3=ax.scatter(X_3[:, 0], X_3[:, 1], X_3[:, 2],c='#ff7f0e',s=10, marker=\"s\", edgecolor='face')\n",
        "x4=ax.scatter(X_4[:, 0], X_4[:, 1], X_4[:, 2],c='#8c564b',s=10, marker=\"v\", edgecolor='face')\n",
        "\n",
        "ax.w_xaxis.set_ticklabels([])\n",
        "ax.w_yaxis.set_ticklabels([])\n",
        "ax.w_zaxis.set_ticklabels([])\n",
        "ax.legend([x1, x2, x3, x4], ['Normal','Ball','Outer Race','InnerRace'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rvd5hgWaSFZ"
      },
      "source": [
        "建立Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueQ0EuW6bWuj"
      },
      "source": [
        "用于原始数据的分类数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbtNtZnYa5ZJ"
      },
      "source": [
        "#用于原始数据的分类数据集\n",
        "class subDataset(Dataset.Dataset):\n",
        "    #初始化，定义数据内容和标签\n",
        "    def __init__(self, Datadf):\n",
        "        self.Data = Datadf['data']\n",
        "        self.Label = Datadf['label']\n",
        "    #返回数据集大小\n",
        "    def __len__(self):\n",
        "        return len(self.Data)\n",
        "    #得到数据内容和标签\n",
        "    def __getitem__(self, index):\n",
        "        #datasize=(1,400)\n",
        "        data = torch.Tensor(self.Data[index]).unsqueeze(0)\n",
        "        label=self.Label[index]\n",
        "        return data, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6A-FhLbZKF"
      },
      "source": [
        "用于计算FID的数据集存储为20*20的图像"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djMtME65aQV7"
      },
      "source": [
        "\n",
        "class imgDataset(Dataset.Dataset):\n",
        "    #初始化，定义数据内容和标签\n",
        "    def __init__(self, Datadf):\n",
        "        #内容\n",
        "        self.Data = Datadf['data']\n",
        "        #标签\n",
        "        self.Label = Datadf['label']\n",
        "\n",
        "    #返回数据集大小\n",
        "    def __len__(self):\n",
        "        return len(self.Data)\n",
        "\n",
        "    #得到数据内容和标签\n",
        "    def __getitem__(self, index):\n",
        "        #datasize=(1,20,20)\n",
        "        data = torch.Tensor(self.Data[index]).reshape(20,20)\n",
        "        label=self.Label[index]\n",
        "        return data, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENUlHpjqaHAA"
      },
      "source": [
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #输入为1，400\n",
        "        #进行BatchNorm\n",
        "        self.batchnorm1= nn.BatchNorm1d(1)\n",
        "        #输入为1，400，一维卷积卷积核大小3，padding 1 步长2 输出为8，200\n",
        "        self.conv2 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3,padding=1,stride=2)\n",
        "        \n",
        "        \n",
        "        #进行BatchNorm\n",
        "        self.batchnorm3= nn.BatchNorm1d(8)\n",
        "        #输入为8，200，一维卷积卷积核大小3，padding 1 步长2 输出为1，100\n",
        "        self.conv3 = nn.Conv1d(in_channels=8, out_channels=1, kernel_size=3,padding=1,stride=2)\n",
        "\n",
        "\n",
        "        #输入为1，100 输出为1，10\n",
        "        self.fc1 = nn.Linear(in_features=100,out_features=10)\n",
        "        #输入为1，100 输出为1，4\n",
        "        self.fc2 = nn.Linear(in_features=10,out_features=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.batchnorm1(x)\n",
        "        #采用relu作为激活函数\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x= self.batchnorm3(x)\n",
        "        #采用relu作为激活函数\n",
        "        x = F.relu(self.conv3(x))\n",
        "        #改变形状为1，100\n",
        "        x = x.view(-1, 100)             \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScfrLntZeHvg"
      },
      "source": [
        "训练过程"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrFPeApteG8Y"
      },
      "source": [
        "def train(epoch):\n",
        "    #首先让网络能训练\n",
        "    net.train()\n",
        "    #从dataloader里面读数据\n",
        "    for data in tqdm(dataloader, leave=False, total=len(dataloader)):\n",
        "        #输入为1，400\n",
        "        inputs=data[0]\n",
        "        #目标标签是0-3之间的数字\n",
        "        targets=data[1]\n",
        "\n",
        "        #这些放在GPU上\n",
        "        inputs=inputs.to(device)\n",
        "        targets=targets.to(device)\n",
        "\n",
        "        #每次喂入数据前，都需要将梯度清零        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #得到输出结果\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        #计算loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        #传回反向梯度\n",
        "        loss.backward()\n",
        "        #梯度传回，利用优化器将参数更新\n",
        "        optimizer.step()\n",
        "    \n",
        "    #打印loss \n",
        "    print(loss)\n",
        "    \n",
        "    #为了可视化返回loss的值\n",
        "    return(loss.item())"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx0PIVh5fBCM"
      },
      "source": [
        "test_loss={}\n",
        "def test(epoch):\n",
        "\n",
        "    #最优的准确度\n",
        "    global best_acc\n",
        "    #让神经网路进行验证\n",
        "    net.eval()\n",
        "    \n",
        "    #初始化\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    best_acc=0\n",
        "\n",
        "\n",
        "    #因为是测试，因此禁止梯度\n",
        "    with torch.no_grad():\n",
        "\n",
        "        #从dataloader里面读数据\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "\n",
        "            #这些放在GPU上\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            #得到输出结果\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            #计算损失\n",
        "            loss = criterion(outputs, targets)            \n",
        "            test_loss += loss.item()\n",
        "\n",
        "            #用于得到准确度\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "\n",
        "    #计算准确度\n",
        "    acc = 100. * correct / total\n",
        "    print(\"Now acc is {}\".format(acc))\n",
        "    #保存准确度最好的值\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "          os.mkdir('checkpoint')\n",
        "          torch.save(state, './checkpoint/ckpt.t7')\n",
        "        best_acc = acc\n",
        "    return acc \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgZ-UAHOiADu"
      },
      "source": [
        "#使用GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#神经网络放在GPU上\n",
        "net = net.to(device)\n",
        "\n",
        "#采用CrossEntropyLoss作为损失函数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#采用Adam作为优化器，学希率为0.001\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J39gpeiqiLOl"
      },
      "source": [
        "train_losses = []\n",
        "accurate = []\n",
        "\n",
        "#迭代100次\n",
        "for epoch in range(0,100):\n",
        "\n",
        "        #保存loss\n",
        "        train_losses.append(train(epoch))\n",
        "        accurate.append(test(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvuXGdSVix0W"
      },
      "source": [
        "#画图可视化\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Train Loss During Training\")\n",
        "plt.plot(train_losses,label=\"train\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig('./CNNloss.jpg')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Train Loss During Training\")\n",
        "plt.plot(train_losses,label=\"train\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig('./GANCNNloss.jpg')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR28EKYSjAMx"
      },
      "source": [
        "#注释参考test()\n",
        "def test_data():\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    best_acc=0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            print(predicted)\n",
        "            print(targets)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    print(\"Now acc is {}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}