{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled34.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCCVNM2aGdaaBzMweqxN0Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanhaodiZhang/HATE/blob/main/GAN%E6%A3%80%E6%B5%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYsIdbNhSgOp"
      },
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA \n",
        "from pandas.core.frame import DataFrame\n",
        "import torch\n",
        "import torch.utils.data.dataset as Dataset\n",
        "import torch.utils.data.dataloader as DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "path = r'data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fax8yVLRkYGB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtKkClIGSjX5"
      },
      "source": [
        "#读数据名\n",
        "filenames = os.listdir(r'data')\n",
        "files = {}\n",
        "#读取每个数据\n",
        "for i in filenames:\n",
        "  #路径\n",
        "  file_path = os.path.join(r'data', i)\n",
        "  #加载\n",
        "  file = loadmat(file_path)\n",
        "  #一个文件中不同类型数据\n",
        "  file_keys = file.keys()\n",
        "  for key in file_keys:\n",
        "    #选择；有DE的数据\n",
        "    if 'DE' in key:\n",
        "      #整理成一行一维数据\n",
        "      files[i] = file[key].ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DNhklReTGmQ"
      },
      "source": [
        "#一个数据1.400\n",
        "length=400\n",
        "#数据取值步长400，就是第一个1-400，第二个400-800\n",
        "step=400\n",
        "#标签从0开始\n",
        "label=0\n",
        "#建立数据集空白的\n",
        "#训练集\n",
        "df_train = pd.DataFrame(columns=('data', 'label','name'))\n",
        "#验证\n",
        "df_val = pd.DataFrame(columns=('data', 'label','name'))\n",
        "#测试\n",
        "df_test = pd.DataFrame(columns=('data', 'label','name'))\n",
        "keys = files.keys()\n",
        "#keys是不同标签的分类\n",
        "for i in keys:\n",
        "  #全部\n",
        "  slice_data = files[i]\n",
        "  #长度\n",
        "  all_lenght = len(slice_data)\n",
        "  #训练结束节点\n",
        "  train_end_index = int(all_lenght*0.6)\n",
        "  #验证结束节点\n",
        "  val_end_index = int(all_lenght*0.8)\n",
        "  start_index=0\n",
        "  #训练结束节点之前切数据成400\n",
        "  while start_index+length <= train_end_index:\n",
        "      #切\n",
        "      sample = slice_data[start_index:start_index + length]\n",
        "      #加400\n",
        "      start_index+=step\n",
        "      #加到数据集上\n",
        "      df_train=df_train.append([{'data':sample,'label':label,'name':i}], ignore_index=True)\n",
        "  \n",
        "  #注释同上\n",
        "  start_index=train_end_index    \n",
        "  while start_index+length <= val_end_index:\n",
        "      sample = slice_data[start_index:start_index + length]\n",
        "      start_index+=step\n",
        "      \n",
        "      df_val=df_val.append([{'data':sample,'label':label,'name':i}], ignore_index=True)\n",
        "      \n",
        "  start_index=val_end_index    \n",
        "  while start_index+length <= all_lenght:\n",
        "      sample = slice_data[start_index:start_index + length]\n",
        "      start_index+=step      \n",
        "      df_test=df_test.append([{'data':sample,'label':label,'name':i}], ignore_index=True)\n",
        "  \n",
        "  label+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxSv1RnUYeO7"
      },
      "source": [
        "打乱数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZwtoOu_YWPb"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "df_train = shuffle(df_train)\n",
        "df_test = shuffle(df_test)\n",
        "df_val = shuffle(df_val)\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "df_val.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8i_-T4GZ1gB"
      },
      "source": [
        "数据可视化（你不需要注释吧）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu9MLh7ZZ0j_"
      },
      "source": [
        "noramlarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "train_nomal=df_train[df_train['label']==1]\n",
        "for arry in train_nomal['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  noramlarry=np.append(noramlarry, arry, axis=0)\n",
        "\n",
        "gundongarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "train_gundong=df_train[df_train['label']==2]\n",
        "for arry in train_gundong['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  gundongarry=np.append(gundongarry, arry, axis=0)\n",
        "\n",
        "outarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "train_out=df_train[df_train['label']==3]\n",
        "for arry in train_out['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  outarry=np.append(outarry, arry, axis=0)\n",
        "\n",
        "inarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "train_in=df_train[df_train['label']==0]\n",
        "for arry in train_in['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  inarry=np.append(inarry, arry, axis=0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiYPEu9RZSzb"
      },
      "source": [
        "allarry=np.expand_dims(df_train['data'][0],axis=0)\n",
        "for arry in df_train['data']:\n",
        "  arry=np.expand_dims(arry,axis=0)\n",
        "  allarry=np.append(allarry, arry, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0qYnAyJaBi5"
      },
      "source": [
        "sklearn.decomposition.PCA\n",
        "pca = PCA(n_components=3)\n",
        "pca=pca.fit(allarry)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzjDpBsnaEKX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(1, figsize=(8, 8))\n",
        "plt.clf()\n",
        "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
        "plt.cla()\n",
        "\n",
        "pca1=pca.fit(noramlarry)\n",
        "X_1 = pca1.transform(noramlarry)\n",
        "pca2=pca.fit(gundongarry)\n",
        "X_2 = pca2.transform(gundongarry)\n",
        "\n",
        "pca3=pca.fit(inarry)\n",
        "X_3 = pca3.transform(inarry)\n",
        "pca4=pca.fit(outarry)\n",
        "X_4 = pca4.transform(outarry)\n",
        "x1=ax.scatter(X_1[:, 0], X_1[:, 1], X_1[:, 2],c='#1f77b4',s=10, marker=\"o\", edgecolor='face')\n",
        "x2=ax.scatter(X_2[:, 0], X_2[:, 1], X_2[:, 2],c='#2ca02c',s=10, marker=\"x\", edgecolor='face')\n",
        "x3=ax.scatter(X_3[:, 0], X_3[:, 1], X_3[:, 2],c='#ff7f0e',s=10, marker=\"s\", edgecolor='face')\n",
        "x4=ax.scatter(X_4[:, 0], X_4[:, 1], X_4[:, 2],c='#8c564b',s=10, marker=\"v\", edgecolor='face')\n",
        "\n",
        "ax.w_xaxis.set_ticklabels([])\n",
        "ax.w_yaxis.set_ticklabels([])\n",
        "ax.w_zaxis.set_ticklabels([])\n",
        "ax.legend([x1, x2, x3, x4], ['Normal','Ball','Outer Race','InnerRace'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rvd5hgWaSFZ"
      },
      "source": [
        "建立GANDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6A_ChY6kf1b"
      },
      "source": [
        "#建立GAN的dataset\n",
        "class GANDataset(Dataset.Dataset):\n",
        "    #初始化，定义数据内容和标签\n",
        "    def __init__(self, Datadf):\n",
        "        self.Data = Datadf['data']\n",
        "        self.Label = Datadf['label']\n",
        "    #返回数据集大小\n",
        "    def __len__(self):\n",
        "        return len(self.Data)\n",
        "    #得到数据内容和标签\n",
        "    def __getitem__(self, index):\n",
        "        data = torch.Tensor(self.Data[index])\n",
        "        label=self.Label[index]\n",
        "        return data, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDBx6hVuYC38"
      },
      "source": [
        "GAN模型建立"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZwtbHSWYJu2"
      },
      "source": [
        "生成器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paDZHMw6ks8M"
      },
      "source": [
        "#一共有四类\n",
        "n_classes=4\n",
        "#噪声的大小为100\n",
        "latent_dim=100\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        #首先对标签进行编码,输出为4.标签输入大小为(1)\n",
        "        self.label_emb = nn.Embedding(n_classes, n_classes)\n",
        "\n",
        "        #每一个block构成\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            #线性层\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            #增加.BatchNorm\n",
        "            layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "\n",
        "            #采用leakyReLU作为激活函数\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            #第一个block,输入1，104 输出 1，128\n",
        "            *block(latent_dim + n_classes, 128, normalize=False),\n",
        "            #第2个block,输入1，128 输出 1，256\n",
        "            *block(128, 256),\n",
        "\n",
        "            #第3个block,输入1，256 输出 1，512\n",
        "            *block(256, 512),\n",
        "\n",
        "            #第4个block,输入1，512 输出 1，1024\n",
        "            *block(512, 1024),\n",
        "            #输入1，1024 输出 1，400\n",
        "            nn.Linear(1024, 400),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        # 用cat将噪声和编码后的标签结合起来\n",
        "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
        "        #得到输出\n",
        "        sample = self.model(gen_input)\n",
        "        #返回1，400的数据\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYwnPT9LYMro"
      },
      "source": [
        "辨别器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noaJGJhAZ6Wl"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "\n",
        "        #首先对标签进行编码,输出为4.标签输入大小为(1)\n",
        "        self.label_embedding = nn.Embedding(n_classes, n_classes)\n",
        "\n",
        "        #建立模型\n",
        "        self.model = nn.Sequential(\n",
        "            #线性层输入400+4，输出512\n",
        "            nn.Linear(n_classes + 400, 512),\n",
        "\n",
        "\n",
        "\n",
        "            #激活函数leakyRelu\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            #线性层输入512，输出512\n",
        "            nn.Linear(512, 512),\n",
        "            #droupout层，正则化\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            #激活函数leakyRelu\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            #线性层输入512，输出512\n",
        "            nn.Linear(512, 512),\n",
        "            #droupout层，正则化\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            \n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "            #线性层输入512，输出1\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, sample, labels):\n",
        "        # 将编码后的label和数据拼接\n",
        "        d_in = torch.cat((sample, self.label_embedding(labels)), -1)\n",
        "        #输出判断是否真实数据\n",
        "        validity = self.model(d_in)\n",
        "        return validity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH2vvxi3hxs9"
      },
      "source": [
        "dataset = GANDataset(df_train)\n",
        "\n",
        "dataloader = DataLoader.DataLoader(dataset,batch_size= 4, shuffle = True, num_workers= 2,drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4jmIXRLiBCB"
      },
      "source": [
        "#损失函数\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "generator.to(device)\n",
        "discriminator.to(device)\n",
        "adversarial_loss.to(device)\n",
        "FloatTensor = torch.cuda.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor\n",
        "\n",
        "#优化器参数\n",
        "lr=0.001\n",
        "b1, b2=0.5,0.999\n",
        "# 优化器\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "\n",
        "#batchsize\n",
        "batch_size=4\n",
        "\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "#训练批次\n",
        "n_epochs=50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "\n",
        "        # 真的输出值是1\n",
        "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
        "        # 假的的值是0\n",
        "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # 真实数据放到GPU上\n",
        "        real_datas = inputs.to(device)\n",
        "        labels = targets.to(device)\n",
        "\n",
        "        # -----------------\n",
        "        #  训练生成器\n",
        "        # -----------------\n",
        "    \n",
        "        #梯度归零\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # 随机生成噪声和标签\n",
        "        #噪声100\n",
        "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
        "        #标签1\n",
        "        gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
        "\n",
        "        # 生成数据400\n",
        "        gen_datas = generator(z, gen_labels)\n",
        "\n",
        "        # 辨别器不更新，但是生成是否真实\n",
        "        validity = discriminator(gen_datas, gen_labels)\n",
        "        # 计算这个时候的损失来更新生成器\n",
        "        g_loss = adversarial_loss(validity, valid)\n",
        "\n",
        "        #更新生成器\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        #辨别器梯度清零\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # 计算真实的数据损失\n",
        "        validity_real = discriminator(real_datas, labels)\n",
        "        d_real_loss = adversarial_loss(validity_real, valid)\n",
        "\n",
        "        # 计算生成的数据损失\n",
        "        validity_fake = discriminator(gen_datas.detach(), gen_labels)\n",
        "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
        "\n",
        "        # 平均得到辨别器损失\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "\n",
        "        #辨别器梯度更新\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        #用于可视化\n",
        "        G_losses.append(g_loss.item())\n",
        "        D_losses.append(d_loss.item())\n",
        "\n",
        "        #打印loss\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "        )\n",
        "        batches_done = epoch * len(dataloader) + i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9YSJ_YNpaJw"
      },
      "source": [
        "数据可视化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh_BJAwTpR4Q"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig('./loss.jpg')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6pzPT7ap3ft"
      },
      "source": [
        "生成1000个数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMj2B-kKpixW"
      },
      "source": [
        "batch_size=1\n",
        "#建立生成的dataframe\n",
        "df_generate = pd.DataFrame(columns=('data', 'label'))\n",
        "#数据存储路径\n",
        "path='fake_img/'\n",
        "\n",
        "#生成数据\n",
        "for i in range(0,1000):\n",
        "  generator.eval()\n",
        "  z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
        "  gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, batch_size)))\n",
        "  gen_datas = generator(z, gen_labels).cpu().detach()\n",
        "  img=gen_datas.reshape(20,20)\n",
        "  df_generate=df_generate.append([{'data':gen_datas,'label':gen_labels.item(),'name':i}], ignore_index=True)\n",
        "  im = unloader(img)\n",
        "  id=str(i)\n",
        "  im.save(path+\"fake_\"+id+\".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rluhm3bJqrlp"
      },
      "source": [
        "数据可视化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jef8M9grqn-L"
      },
      "source": [
        "df_train2=df_generate\n",
        "noramlarry=df_train2['data'][0]\n",
        "print(noramlarry.shape)\n",
        "train_nomal=df_train2[df_train2['label']==1]\n",
        "for arry in train_nomal['data']:\n",
        "  noramlarry=np.append(noramlarry, arry, axis=0)\n",
        "\n",
        "gundongarry=df_train2['data'][0]\n",
        "train_gundong=df_train2[df_train2['label']==2]\n",
        "for arry in train_gundong['data']:\n",
        "  gundongarry=np.append(gundongarry, arry, axis=0)\n",
        "\n",
        "outarry=df_train2['data'][0]\n",
        "train_out=df_train2[df_train2['label']==3]\n",
        "for arry in train_out['data']:\n",
        "  outarry=np.append(outarry, arry, axis=0)\n",
        "\n",
        "inarry=df_train2['data'][0]\n",
        "train_in=df_train2[df_train2['label']==0]\n",
        "for arry in train_in['data']:\n",
        "  inarry=np.append(inarry, arry, axis=0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B3JoQAWqyXW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(1, figsize=(8, 8))\n",
        "plt.clf()\n",
        "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
        "plt.cla()\n",
        "\n",
        "pca1=pca.fit(noramlarry)\n",
        "X_1 = pca1.transform(noramlarry)\n",
        "pca2=pca.fit(gundongarry)\n",
        "X_2 = pca2.transform(gundongarry)\n",
        "\n",
        "pca3=pca.fit(inarry)\n",
        "X_3 = pca3.transform(inarry)\n",
        "pca4=pca.fit(outarry)\n",
        "X_4 = pca4.transform(outarry)\n",
        "x1=ax.scatter(X_1[:, 0], X_1[:, 1], X_1[:, 2],c='#1f77b4',s=10, marker=\"o\", edgecolor='face')\n",
        "x2=ax.scatter(X_2[:, 0], X_2[:, 1], X_2[:, 2],c='#2ca02c',s=10, marker=\"x\", edgecolor='face')\n",
        "x3=ax.scatter(X_3[:, 0], X_3[:, 1], X_3[:, 2],c='#ff7f0e',s=10, marker=\"s\", edgecolor='face')\n",
        "x4=ax.scatter(X_4[:, 0], X_4[:, 1], X_4[:, 2],c='#8c564b',s=10, marker=\"v\", edgecolor='face')\n",
        "\n",
        "ax.w_xaxis.set_ticklabels([])\n",
        "ax.w_yaxis.set_ticklabels([])\n",
        "ax.w_zaxis.set_ticklabels([])\n",
        "ax.legend([x1, x2, x3, x4], ['Normal','Ball','Outer Race','InnerRace'])\n",
        "plt.savefig('./test2.jpg')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMRV9pR-vMo7"
      },
      "source": [
        "计算FID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvras6yvMTf"
      },
      "source": [
        "!pip install pytorch-fid\n",
        "!python -m pytorch_fid fake_img real_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueQ0EuW6bWuj"
      },
      "source": [
        "用于原始数据的分类数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbtNtZnYa5ZJ"
      },
      "source": [
        "#用于原始数据的分类数据集\n",
        "class subDataset(Dataset.Dataset):\n",
        "    #初始化，定义数据内容和标签\n",
        "    def __init__(self, Datadf):\n",
        "        self.Data = Datadf['data']\n",
        "        self.Label = Datadf['label']\n",
        "    #返回数据集大小\n",
        "    def __len__(self):\n",
        "        return len(self.Data)\n",
        "    #得到数据内容和标签\n",
        "    def __getitem__(self, index):\n",
        "        #datasize=(1,400)\n",
        "        data = torch.Tensor(self.Data[index]).unsqueeze(0)\n",
        "        label=self.Label[index]\n",
        "        return data, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6A-FhLbZKF"
      },
      "source": [
        "用于计算FID的数据集存储为20*20的图像"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djMtME65aQV7"
      },
      "source": [
        "\n",
        "class imgDataset(Dataset.Dataset):\n",
        "    #初始化，定义数据内容和标签\n",
        "    def __init__(self, Datadf):\n",
        "        #内容\n",
        "        self.Data = Datadf['data']\n",
        "        #标签\n",
        "        self.Label = Datadf['label']\n",
        "\n",
        "    #返回数据集大小\n",
        "    def __len__(self):\n",
        "        return len(self.Data)\n",
        "\n",
        "    #得到数据内容和标签\n",
        "    def __getitem__(self, index):\n",
        "        #datasize=(1,20,20)\n",
        "        data = torch.Tensor(self.Data[index]).reshape(20,20)\n",
        "        label=self.Label[index]\n",
        "        return data, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KZ-s9qZkODf"
      },
      "source": [
        "CNN模型建立"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENUlHpjqaHAA"
      },
      "source": [
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #输入为1，400\n",
        "        #进行BatchNorm\n",
        "        self.batchnorm1= nn.BatchNorm1d(1)\n",
        "        #输入为1，400，一维卷积卷积核大小3，padding 1 步长2 输出为8，200\n",
        "        self.conv2 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3,padding=1,stride=2)\n",
        "        \n",
        "        \n",
        "        #进行BatchNorm\n",
        "        self.batchnorm3= nn.BatchNorm1d(8)\n",
        "        #输入为8，200，一维卷积卷积核大小3，padding 1 步长2 输出为1，100\n",
        "        self.conv3 = nn.Conv1d(in_channels=8, out_channels=1, kernel_size=3,padding=1,stride=2)\n",
        "\n",
        "\n",
        "        #输入为1，100 输出为1，10\n",
        "        self.fc1 = nn.Linear(in_features=100,out_features=10)\n",
        "        #输入为1，100 输出为1，4\n",
        "        self.fc2 = nn.Linear(in_features=10,out_features=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.batchnorm1(x)\n",
        "        #采用relu作为激活函数\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x= self.batchnorm3(x)\n",
        "        #采用relu作为激活函数\n",
        "        x = F.relu(self.conv3(x))\n",
        "        #改变形状为1，100\n",
        "        x = x.view(-1, 100)             \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmnICM1SrGLQ"
      },
      "source": [
        "合并生成数据和原始数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkwdsMrWrFXh"
      },
      "source": [
        "dataset = subDataset(df_train)\n",
        "dloader = DataLoader.DataLoader(dataset,batch_size= 1, shuffle = True, num_workers= 2)\n",
        "\n",
        "for batch_idx, (inputs, targets) in enumerate(dloader):\n",
        "  gen_datas=inputs.reshape(1,400)\n",
        "  df_generate=df_generate.append([{'data':gen_datas,'label':targets.item()}], ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsUMgd4IsmLZ"
      },
      "source": [
        "训练数据是混合数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "731Y4XTHsbmC"
      },
      "source": [
        "nowdataset = GANDataset(df_generate)\n",
        "generatedataloader = DataLoader.DataLoader(nowdataset,batch_size= 4, shuffle = True, num_workers= 2,drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D536BzRsrTR"
      },
      "source": [
        "验证数据是非混合数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMQxBFzssozb"
      },
      "source": [
        "testset = subDataset(df_val)\n",
        "testloader = DataLoader.DataLoader(testset,batch_size= 4, shuffle = True, num_workers= 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWBp_PamtN5f"
      },
      "source": [
        "建立CNN网络参考CNN代码\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0HG4gHQsjrL"
      },
      "source": [
        "net=Net()\n",
        "net = net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScfrLntZeHvg"
      },
      "source": [
        "训练过程"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrFPeApteG8Y"
      },
      "source": [
        "def train(epoch):\n",
        "    #首先让网络能训练\n",
        "    net.train()\n",
        "    #从dataloader里面读数据\n",
        "    for data in tqdm(generatedataloader, leave=False, total=len(dataloader)):\n",
        "        #输入为1，400\n",
        "        inputs=data[0]\n",
        "        #目标标签是0-3之间的数字\n",
        "        targets=data[1]\n",
        "\n",
        "        #这些放在GPU上\n",
        "        inputs=inputs.to(device)\n",
        "        targets=targets.to(device)\n",
        "\n",
        "        #每次喂入数据前，都需要将梯度清零        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #得到输出结果\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        #计算loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        #传回反向梯度\n",
        "        loss.backward()\n",
        "        #梯度传回，利用优化器将参数更新\n",
        "        optimizer.step()\n",
        "    \n",
        "    #打印loss \n",
        "    print(loss)\n",
        "\n",
        "    #为了可视化返回loss的值\n",
        "    return(loss.item())"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx0PIVh5fBCM"
      },
      "source": [
        "test_loss={}\n",
        "def test(epoch):\n",
        "\n",
        "    #最优的准确度\n",
        "    global best_acc\n",
        "    #让神经网路进行验证\n",
        "    net.eval()\n",
        "    \n",
        "    #初始化\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    best_acc=0\n",
        "\n",
        "\n",
        "    #因为是测试，因此禁止梯度\n",
        "    with torch.no_grad():\n",
        "\n",
        "        #从dataloader里面读数据\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "\n",
        "            #这些放在GPU上\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            #得到输出结果\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            #计算损失\n",
        "            loss = criterion(outputs, targets)            \n",
        "            test_loss += loss.item()\n",
        "\n",
        "            #用于得到准确度\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "\n",
        "    #计算准确度\n",
        "    acc = 100. * correct / total\n",
        "    print(\"Now acc is {}\".format(acc))\n",
        "    #保存准确度最好的值\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "          os.mkdir('checkpoint')\n",
        "          torch.save(state, './checkpoint/ckpt.t7')\n",
        "        best_acc = acc\n",
        "    return acc \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J39gpeiqiLOl"
      },
      "source": [
        "train_losses = []\n",
        "accurate = []\n",
        "\n",
        "#迭代100次\n",
        "for epoch in range(0,100):\n",
        "\n",
        "        #保存loss\n",
        "        train_losses.append(train(epoch))\n",
        "        accurate.append(test(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvuXGdSVix0W"
      },
      "source": [
        "#画图可视化\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Train Loss During Training\")\n",
        "plt.plot(train_losses,label=\"train\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig('./CNNloss.jpg')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR28EKYSjAMx"
      },
      "source": [
        "#注释参考test()\n",
        "def test_data():\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    best_acc=0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            print(predicted)\n",
        "            print(targets)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    print(\"Now acc is {}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}