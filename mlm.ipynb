{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled29.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOue5u0Hiqvcf4Jt2HCARq9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd24c234f0504aeea9aff569fe1f6843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b01dc6846c945b49c5c745b364d5d55",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d9d919047ebb4d2e8168055436757e64",
              "IPY_MODEL_ef7a25167e2540508d07e5126377d264"
            ]
          }
        },
        "3b01dc6846c945b49c5c745b364d5d55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9d919047ebb4d2e8168055436757e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d1b4f4a21a274598a22775976db26b8b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 13240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9383def6b73b44648d3892fac2281ace"
          }
        },
        "ef7a25167e2540508d07e5126377d264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_433dac8807d348ef9a027f7f264df421",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13240/13240 [05:23&lt;00:00, 40.93it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff5e2f95326b4289bf51845eb38905d4"
          }
        },
        "d1b4f4a21a274598a22775976db26b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9383def6b73b44648d3892fac2281ace": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "433dac8807d348ef9a027f7f264df421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff5e2f95326b4289bf51845eb38905d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanhaodiZhang/HATE/blob/main/mlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4sqmvfWIJ8K"
      },
      "source": [
        "!pip install -qq transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cPtIS8mR937"
      },
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "VlY-g6rWR_pr",
        "outputId": "82c5b6bf-1c92-40db-86ff-63cae59dda0e"
      },
      "source": [
        "df = pd.read_csv(\"olid-training.tsv\", sep='\\t', header=0, index_col='id')\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>86426</th>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90194</th>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16820</th>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62688</th>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43605</th>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet  ... subtask_c\n",
              "id                                                        ...          \n",
              "86426  @USER She should ask a few native Americans wh...  ...       NaN\n",
              "90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       IND\n",
              "16820  Amazon is investigating Chinese employees who ...  ...       NaN\n",
              "62688  @USER Someone should'veTaken\" this piece of sh...  ...       NaN\n",
              "43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "7BDMHh8USAiK",
        "outputId": "4cd452d2-e1cd-4626-b9b5-442f21bf4e9b"
      },
      "source": [
        "df_test_label = pd.read_csv(\"labels-levela.csv\",header=None,names=['id','subtask_a'],index_col='id')\r\n",
        "df_test_tweet = pd.read_csv(\"testset-levela.tsv\", sep='\\t', header=0, index_col='id')\r\n",
        "df_test = pd.merge(df_test_tweet, df_test_label, on=['id'])\r\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15923</th>\n",
              "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27014</th>\n",
              "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30530</th>\n",
              "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13876</th>\n",
              "      <td>#Watching #Boomer getting the news that she is...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60133</th>\n",
              "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet subtask_a\n",
              "id                                                                \n",
              "15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...       OFF\n",
              "27014  #ConstitutionDay is revered by Conservatives, ...       NOT\n",
              "30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...       NOT\n",
              "13876  #Watching #Boomer getting the news that she is...       NOT\n",
              "60133  #NoPasaran: Unity demo to oppose the far-right...       OFF"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLZPnOX2SFSu"
      },
      "source": [
        "df['comment']= df['tweet']\r\n",
        "df_test['comment'] = df_test['tweet']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS4ZRSSSSGyS"
      },
      "source": [
        "\r\n",
        "# M = df['comment']\r\n",
        " \r\n",
        "# M.to_csv('train.txt',header=False,index=False)\r\n",
        "# M = df_test['comment']\r\n",
        "# M.to_csv('val.txt', header=False, index=False)\r\n",
        "\r\n",
        "with open(\"train.txt\", 'w') as f:\r\n",
        "    for line in df['comment'].to_list():\r\n",
        "        f.write(line + '\\n')\r\n",
        "\r\n",
        "with open(\"val.txt\", 'w') as f:\r\n",
        "    for line in df_test['comment'].to_list():\r\n",
        "        f.write(line + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDzN5_O_SI7l"
      },
      "source": [
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\r\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\r\n",
        "# tokenizer.add_special_tokens({'additional_special_tokens':[\"<OFF>\",\"<NOT>\"]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7xbqxDwSI3Z",
        "outputId": "3db3b59d-e5d7-45af-cfd5-923ab537766f"
      },
      "source": [
        "from transformers import BertForMaskedLM\r\n",
        "\r\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\r\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSxT15kASL7t",
        "outputId": "0bb2055d-9f30-459a-95fa-3abe1ad08cb8"
      },
      "source": [
        "# TRAIN ONLY\r\n",
        "from transformers import LineByLineTextDataset\r\n",
        "from transformers import DataCollatorForLanguageModeling\r\n",
        "\r\n",
        "train_dataset = LineByLineTextDataset(\r\n",
        "    tokenizer=tokenizer,\r\n",
        "    file_path=\"train.txt\",\r\n",
        "    block_size=128,\r\n",
        ")\r\n",
        "\r\n",
        "valid_dataset = LineByLineTextDataset(\r\n",
        "    tokenizer=tokenizer,\r\n",
        "    file_path=\"val.txt\",\r\n",
        "    block_size=128,\r\n",
        ")\r\n",
        "\r\n",
        "data_collator = DataCollatorForLanguageModeling(\r\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/datasets/language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu4wYuoJSL3u"
      },
      "source": [
        "# TRAIN ONLY\r\n",
        "from transformers import Trainer, TrainingArguments, EvalPrediction\r\n",
        "\r\n",
        "def compute_metrics(p: EvalPrediction):\r\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\r\n",
        "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\r\n",
        "    \r\n",
        "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\r\n",
        "\r\n",
        "model.cuda()\r\n",
        "training_args = TrainingArguments(\r\n",
        "    output_dir=\"./mlm3\",\r\n",
        "    overwrite_output_dir=True,\r\n",
        "    num_train_epochs=15,\r\n",
        "    per_device_train_batch_size=16,\r\n",
        "    save_steps=1000,\r\n",
        "    save_total_limit=10,\r\n",
        "    evaluation_strategy='epoch',\r\n",
        "    learning_rate=1e-4,\r\n",
        "    weight_decay=1e-5,\r\n",
        "    lr_scheduler_type='cosine',\r\n",
        "    warmup_steps=500,\r\n",
        "    metric_for_best_model='eval_loss',\r\n",
        "    load_best_model_at_end=True,\r\n",
        "    disable_tqdm=True,\r\n",
        ")\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "    model=model,\r\n",
        "    args=training_args,\r\n",
        "    data_collator=data_collator,\r\n",
        "    train_dataset=train_dataset,\r\n",
        "    eval_dataset=valid_dataset,\r\n",
        "    # compute_metrics=compute_metrics\r\n",
        "    # prediction_loss_only=True,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKaz6rV1SQB4",
        "outputId": "ec55249f-a7d0-4443-b9cb-3319caeb6dee"
      },
      "source": [
        "%%time\r\n",
        "# TRAIN ONLY\r\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'loss': 2.3263, 'learning_rate': 0.0001, 'epoch': 0.6}\n",
            "{'eval_loss': 1.8276678323745728, 'eval_runtime': 1.7615, 'eval_samples_per_second': 488.217, 'epoch': 1.0}\n",
            "{'loss': 1.1274, 'learning_rate': 9.956649043661442e-05, 'epoch': 1.21}\n",
            "{'loss': 1.0868, 'learning_rate': 9.827347896811953e-05, 'epoch': 1.81}\n",
            "{'eval_loss': 1.8446189165115356, 'eval_runtime': 1.7965, 'eval_samples_per_second': 478.715, 'epoch': 2.0}\n",
            "{'loss': 1.0815, 'learning_rate': 9.614338690800175e-05, 'epoch': 2.42}\n",
            "{'eval_loss': 1.824037790298462, 'eval_runtime': 1.9383, 'eval_samples_per_second': 443.679, 'epoch': 3.0}\n",
            "{'loss': 1.0419, 'learning_rate': 9.321315086741916e-05, 'epoch': 3.02}\n",
            "{'loss': 0.995, 'learning_rate': 8.953358226023457e-05, 'epoch': 3.62}\n",
            "{'eval_loss': 1.7677382230758667, 'eval_runtime': 1.7833, 'eval_samples_per_second': 482.239, 'epoch': 4.0}\n",
            "{'loss': 0.9903, 'learning_rate': 8.516848621366188e-05, 'epoch': 4.23}\n",
            "{'loss': 0.9623, 'learning_rate': 8.019355516295254e-05, 'epoch': 4.83}\n",
            "{'eval_loss': 1.7063571214675903, 'eval_runtime': 1.7676, 'eval_samples_per_second': 486.543, 'epoch': 5.0}\n",
            "{'loss': 0.9061, 'learning_rate': 7.469505631561317e-05, 'epoch': 5.43}\n",
            "{'eval_loss': 1.844009518623352, 'eval_runtime': 1.999, 'eval_samples_per_second': 430.206, 'epoch': 6.0}\n",
            "{'loss': 0.9114, 'learning_rate': 6.876833574502728e-05, 'epoch': 6.04}\n",
            "{'loss': 0.8675, 'learning_rate': 6.251616505306933e-05, 'epoch': 6.64}\n",
            "{'eval_loss': 1.7065657377243042, 'eval_runtime': 2.0091, 'eval_samples_per_second': 428.047, 'epoch': 7.0}\n",
            "{'loss': 0.8516, 'learning_rate': 5.604695927121468e-05, 'epoch': 7.25}\n",
            "{'loss': 0.8386, 'learning_rate': 4.947289690242102e-05, 'epoch': 7.85}\n",
            "{'eval_loss': 1.7429029941558838, 'eval_runtime': 1.8008, 'eval_samples_per_second': 477.574, 'epoch': 8.0}\n",
            "{'loss': 0.7978, 'learning_rate': 4.290797470297501e-05, 'epoch': 8.45}\n",
            "{'eval_loss': 1.7485212087631226, 'eval_runtime': 1.8744, 'eval_samples_per_second': 458.812, 'epoch': 9.0}\n",
            "{'loss': 0.7695, 'learning_rate': 3.6466030935130305e-05, 'epoch': 9.06}\n",
            "{'loss': 0.749, 'learning_rate': 3.0258771368093024e-05, 'epoch': 9.66}\n",
            "{'eval_loss': 1.7003483772277832, 'eval_runtime': 1.9247, 'eval_samples_per_second': 446.822, 'epoch': 10.0}\n",
            "{'loss': 0.7435, 'learning_rate': 2.4393832257252252e-05, 'epoch': 10.27}\n",
            "{'loss': 0.7333, 'learning_rate': 1.8972913890336953e-05, 'epoch': 10.87}\n",
            "{'eval_loss': 1.6759306192398071, 'eval_runtime': 1.78, 'eval_samples_per_second': 483.144, 'epoch': 11.0}\n",
            "{'loss': 0.6817, 'learning_rate': 1.4090017065522732e-05, 'epoch': 11.47}\n",
            "{'eval_loss': 1.735120177268982, 'eval_runtime': 1.7972, 'eval_samples_per_second': 478.512, 'epoch': 12.0}\n",
            "{'loss': 0.707, 'learning_rate': 9.829813081632872e-06, 'epoch': 12.08}\n",
            "{'loss': 0.6998, 'learning_rate': 6.266175505426958e-06, 'epoch': 12.68}\n",
            "{'eval_loss': 1.6861408948898315, 'eval_runtime': 1.8052, 'eval_samples_per_second': 476.408, 'epoch': 13.0}\n",
            "{'loss': 0.6632, 'learning_rate': 3.46089917569401e-06, 'epoch': 13.29}\n",
            "{'loss': 0.6824, 'learning_rate': 1.4626286571091663e-06, 'epoch': 13.89}\n",
            "{'eval_loss': 1.727278470993042, 'eval_runtime': 1.7839, 'eval_samples_per_second': 482.093, 'epoch': 14.0}\n",
            "{'loss': 0.6729, 'learning_rate': 3.060147248739376e-07, 'epoch': 14.49}\n",
            "{'eval_loss': 1.734412431716919, 'eval_runtime': 1.7758, 'eval_samples_per_second': 484.284, 'epoch': 15.0}\n",
            "{'train_runtime': 1801.9626, 'train_samples_per_second': 6.892, 'epoch': 15.0}\n",
            "CPU times: user 24min 32s, sys: 4min 53s, total: 29min 25s\n",
            "Wall time: 30min 2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=12420, training_loss=0.9036793314124459, metrics={'train_runtime': 1801.9626, 'train_samples_per_second': 6.892, 'epoch': 15.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_yH6ZrLH9Af",
        "outputId": "0657a887-cd72-44f7-a0a6-09f94e5962c8"
      },
      "source": [
        "import torch.nn.functional as F\r\n",
        "import torch\r\n",
        "from transformers import BertTokenizer\r\n",
        "from transformers import BertForMaskedLM\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\r\n",
        "model = BertForMaskedLM.from_pretrained(PRE_TRAINED_MODEL_NAME)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP51S4ZsQSUi",
        "outputId": "b90c677e-5ed4-4175-ba89-4dd1f65dcdcd"
      },
      "source": [
        "model.load_state_dict(torch.load(\"./mlm3/pytorch_model.bin\")) # model path\r\n",
        "model.to('cuda')\r\n",
        "model.eval()\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\r\n",
        "tokenizer.add_special_tokens({'additional_special_tokens':[\"@USER\"]}) # add @USER to avoid cutting into '@ USER'\r\n",
        "\r\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30523, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi20qaqPcG05",
        "outputId": "79ee553c-4e48-422f-85b7-a94e09e0d284"
      },
      "source": [
        "# top=5 限制最大概率的5个词，生成时随机从其中sample\r\n",
        "def get_prediction(model, masked_input, tokenizer, top=5):\r\n",
        "    tokenized_inputs = tokenizer(masked_input, return_tensors='pt', add_special_tokens=True, return_attention_mask=True)\r\n",
        "\r\n",
        "    mask_id = tokenizer.mask_token_id\r\n",
        "\r\n",
        "    input_ids = tokenized_inputs['input_ids'].to('cuda')\r\n",
        "    token_type_ids = tokenized_inputs['token_type_ids'].to('cuda')\r\n",
        "    attention_mask = tokenized_inputs['attention_mask'].to('cuda')\r\n",
        "\r\n",
        "    mask_positions = input_ids == mask_id\r\n",
        "\r\n",
        "    result = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\r\n",
        "\r\n",
        "    logits = result.logits\r\n",
        "    \r\n",
        "    mask_logits = F.softmax(logits[mask_positions], dim=-1)\r\n",
        "    mask_preds = torch.argsort(mask_logits, dim=-1, descending=True)\r\n",
        "    mask_preds = mask_preds[:, 0:top].tolist()\r\n",
        "    \r\n",
        "    predition_tokens = []\r\n",
        "    for preds in mask_preds:\r\n",
        "        tokens = tokenizer.convert_ids_to_tokens(preds)\r\n",
        "        tokens = [i for i in tokens if i != '[UNK]'] # remove UNK\r\n",
        "        predition_tokens.append(tokens)\r\n",
        "    return predition_tokens\r\n",
        "\r\n",
        "get_prediction(model, \"You are [MASK]\", tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['gorgeous', 'beautiful', 'amazing', 'adorable']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-phiyuGQ4hG",
        "outputId": "17d5332b-0b91-4471-b13a-220e4eb5d0ec"
      },
      "source": [
        "import random\r\n",
        "random.seed(17)\r\n",
        "import copy\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from string import punctuation\r\n",
        "STOPWORDS = set(stopwords.words('english')) | set(punctuation) | {'@USER', '[UNK]'}\r\n",
        "\r\n",
        "\r\n",
        "# randomly mask noun\r\n",
        "def mask_without_stopwords(text, n=1):\r\n",
        "    mask_candidate_positions = []\r\n",
        "    stop_word_positions = []\r\n",
        "    words = []\r\n",
        "    \r\n",
        "    for i, word in enumerate(tokenizer.tokenize(text)):\r\n",
        "        if word not in STOPWORDS:\r\n",
        "            mask_candidate_positions.append(i)\r\n",
        "        else:\r\n",
        "            stop_word_positions.append(i)\r\n",
        "        words.append(word)\r\n",
        "    \r\n",
        "    if len(mask_candidate_positions) < n:\r\n",
        "        mask_candidate_positions += random.sample(stop_word_positions, n - len(mask_candidate_positions))\r\n",
        "\r\n",
        "    mask_positions = random.sample(mask_candidate_positions, n)\r\n",
        "    mask_words=[]\r\n",
        "    for mask_pos in mask_positions:\r\n",
        "        mask_words.append(words[mask_pos])\r\n",
        "        words[mask_pos] = '[MASK]'\r\n",
        "        \r\n",
        "        \r\n",
        "    return tokenizer.convert_tokens_to_string(words),mask_words\r\n",
        "\r\n",
        "def sample_MLM_prediction(text,maskwords,predictions, K=1):\r\n",
        "    result = []\r\n",
        "    mask_preds = []\r\n",
        "\r\n",
        "    for prediction in predictions[0]:\r\n",
        "        if prediction != maskwords[0]:\r\n",
        "          mask_preds.append(prediction)\r\n",
        "    \r\n",
        "    # for i in range(K):\r\n",
        "    #     temp = text.split()\r\n",
        "    #     tweet=''\r\n",
        "    #     for word in temp:\r\n",
        "    #         if word== maskwords:\r\n",
        "    #           word= mask_preds[mask_num]\r\n",
        "    #         tweet = \" \".join(word)   \r\n",
        "    #     result.append(tweet)\r\n",
        "    try:\r\n",
        "      for i in range(K):\r\n",
        "        temp = text.split()\r\n",
        "        tweet=''\r\n",
        "        tweet_list=[]\r\n",
        "        for word in temp:\r\n",
        "          if word== maskwords[0]:\r\n",
        "\r\n",
        "            word= mask_preds[0][i]\r\n",
        "          tweet_list.append(word)\r\n",
        "        tweet = \" \".join(tweet_list)   \r\n",
        "        result.append(tweet)\r\n",
        "    except IndexError as error:\r\n",
        "      pass\r\n",
        "\r\n",
        "    return result\r\n",
        "\r\n",
        "text='@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Shush it makes the ship make more sense'\r\n",
        "masked_text,maskwords = mask_without_stopwords('@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Shush it makes the ship make more sense', n=1)\r\n",
        "predictions = get_prediction(model, masked_text, tokenizer)\r\n",
        "aug = sample_MLM_prediction(text,maskwords, predictions, 2)\r\n",
        "print(masked_text)\r\n",
        "print(\"----------------------------------------\")\r\n",
        "print(predictions)\r\n",
        "print(\"----------------------------------------\")\r\n",
        "print(aug)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER shush it makes the ship [MASK] more sense\n",
            "----------------------------------------\n",
            "[['make', 'making', 'have', 'no', 'even']]\n",
            "----------------------------------------\n",
            "['@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Shush it makes the ship m more sense', '@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Shush it makes the ship a more sense']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxw4GIKqkB7n"
      },
      "source": [
        "masked_text,maskwords = mask_without_stopwords('@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Shush it makes the ship make more sense', n=1)\r\n",
        "predictions = get_prediction(model, masked_text, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q97uPSJwqZj_"
      },
      "source": [
        "N=1\r\n",
        "K=2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KKhD11uqfTF"
      },
      "source": [
        "df_aug = pd.DataFrame({\r\n",
        "    'id':[],\r\n",
        "    'tweet':[],\r\n",
        "    'subtask_a':[],\r\n",
        "})\r\n",
        "df_aug = df_aug.set_index('id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "fd24c234f0504aeea9aff569fe1f6843",
            "3b01dc6846c945b49c5c745b364d5d55",
            "d9d919047ebb4d2e8168055436757e64",
            "ef7a25167e2540508d07e5126377d264",
            "d1b4f4a21a274598a22775976db26b8b",
            "9383def6b73b44648d3892fac2281ace",
            "433dac8807d348ef9a027f7f264df421",
            "ff5e2f95326b4289bf51845eb38905d4"
          ]
        },
        "id": "B2x81VQlkwq_",
        "outputId": "0459c936-4d35-4040-b081-004d5c7695e0"
      },
      "source": [
        "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\r\n",
        "    label = row['subtask_a'] # use same label\r\n",
        "    masked_tweet,maskwords  = mask_without_stopwords(row['tweet'], N)\r\n",
        "\r\n",
        "    mask_predictions = get_prediction(model, masked_tweet, tokenizer)\r\n",
        "\r\n",
        "    # print([masked_tweet])\r\n",
        "    # print(mask_predictions)\r\n",
        "    augmented_texts = sample_MLM_prediction(row['tweet'],maskwords, mask_predictions, K)\r\n",
        "    \r\n",
        "    for i, text in enumerate(augmented_texts):\r\n",
        "        id = int(index*10 + i + 1)\r\n",
        "        df_aug = df_aug.append([{'id':id,'tweet':text.replace(\" ##\", ''),'subtask_a':label}],ignore_index=False)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd24c234f0504aeea9aff569fe1f6843",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=13240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol3lsfbQzyw6"
      },
      "source": [
        "df_aug = df_aug.set_index('id')\r\n",
        "df_aug.to_csv(\"bert_aug_no_tag_N={}_k={}.csv\".format(N,K))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ3PZ_zOkDOf",
        "outputId": "3ef5815a-b894-4cce-8183-4ff12642d40f"
      },
      "source": [
        "result = []\r\n",
        "mask_preds = []\r\n",
        "for sorted_prediction in predictions:\r\n",
        "        preds = random.sample(sorted_prediction, 2)\r\n",
        "        mask_preds.append(preds)\r\n",
        "mask_preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['made', 'make']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpfRN_6lHx_w",
        "outputId": "07c40d42-5e96-4f4f-d11a-8ff2d553a751"
      },
      "source": [
        "maskwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['make']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB4tSJptc07N"
      },
      "source": [
        "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\r\n",
        "    label = row['subtask_a'] # use same label\r\n",
        "    masked_tweet = mask_without_stopwords(row['tweet'], N)\r\n",
        "\r\n",
        "    mask_predictions, mask_words = get_prediction(model, masked_tweet, tokenizer)\r\n",
        "\r\n",
        "    # print([masked_tweet])\r\n",
        "    # print(mask_predictions)\r\n",
        "    augmented_texts = sample_MLM_prediction(masked_tweet, mask_predictions, K)\r\n",
        "    \r\n",
        "    for i, text in enumerate(augmented_texts):\r\n",
        "        id = int(index*10 + i + 1)\r\n",
        "        df_aug = df_aug.append([{'id':id,'tweet':text.replace(\" ##\", ''),'subtask_a':label}],ignore_index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}